<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hive常见命令</title>
    <url>/LY4E999.github.io/2020/06/20/Hive%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>创建各种表的常规命令</p>
<a id="more"></a>

<p>hadoop dfsadmin -safemode leave    退出安全模式</p>
<p>show databases; 展示数据库</p>
<p>use cs176;  使用cs176数据库</p>
<p>创建一张内部表t_order，原始文件分隔符为’\t’<br>create table t_order (id int,name string) row format delimited fields terminated by ‘\t’;</p>
<p>desc t_order; 展示数据表中字段</p>
<p>load data local inpath ‘/home/order.log’ into table t_order;  传入数据到表t_order</p>
<p>创建一个外部表order_log 表目录在/test下<br>create external table order_log(a int,b string, c int,d int,e int,f int,g int,h double,i int,j string,k string) row format delimited fields terminated by ‘\t’  location ‘/test’;（外部表传入数据直接把文件传到/test目录下即可）</p>
<p>创建内部分区表<br>create table member_address(a int,b int,c string,d string,f string,g string,h string,i string,j string)partitioned by (address string)row format delimited fields terminated by ‘\t’;</p>
<p>传入数据<br>load data local inpath ‘/home/member_address_hlj’ overwrite into table member_adress partition(address=”heilongjiang”);   （overwrite 覆盖   partition(address=”heilongjiang”) 分区）</p>
<p>创建一个外部分区表<br>create external table ext_member_address(a int,b int,c string,d string,f string,g string,h string,i string,j string)partitioned by (address string)row format delimited fields terminated by ‘\t’ location ‘/test’;</p>
<p>添加一个分区<br>alter table ext_member_address add partition(address=”heilongjiang”)location ‘/member_address/heilongjiang’;</p>
<p>打开桶表<br>set hive.enforce.bucketing=true;</p>
<p>创建桶表<br>create table if not exists bk_order_log(a int,b string, c int,d int,e int,f int,g int,h double,i int,j string,k string)clustered by (a) into 4 buckets row format delimited fields terminated by ‘\t’;</p>
<p>加载数据<br> insert into table bk_order_log select *from order_log;</p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/LY4E999.github.io/2020/06/20/Hive/</url>
    <content><![CDATA[<p>目标：</p>
<blockquote>
<blockquote>
<p>掌握Hive的环境部署（见部署文档）<br>理解Hive的基本原理及执行流程<br>掌握HiveQL的基本使用<br>理解Hive内部表、外部表、分区表和桶表<br>能够利用Hive进行数据分析</p>
</blockquote>
</blockquote>
<a id="more"></a>





<p>第1节  环境部署</p>
<p>1.1  环境准备<br>Hadoop版本：2.x.y（包含hdfs和hadoop）<br>JDK 1.8版本<br>Hive 2.x.y版本</p>
<p>1.2  Hive部署<br>为了方便学生完成相关的实验，避免将安装作为主要学习方向，我们使用Cloudera公    司提供的大数据整合开发平台。</p>
<p>链接：<a href="https://pan.baidu.com/s/1Mndli3I-P0ugAPQNhoom5Q" target="_blank" rel="noopener">https://pan.baidu.com/s/1Mndli3I-P0ugAPQNhoom5Q</a><br>提取码：h4b4</p>
<p>第2节  Hive</p>
<p>2.1 简介<br>Hive是基于Hadoop的数据仓库工具。<br>那为什么说Hive是基于Hadoop的呢？<br>简单的说是因为：<br>       ①数据存储在HDFS上<br>       ②数据计算用MapReduce<br>下面我们来深入分析一下：<br>        Hive是一种建立在Hadoop文件系统上的数据仓库架构，并对存储在HDFS中的数据    进行分析和管理；它可以将结构化的数据文件映射为一张数据库表，并提供完整的 SQL     查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行，通过自己的 SQL     去查询分析需要的内容，这套 SQL 简称 Hive SQL（HQL），使不熟悉 MapReduce     的用户也能很方便地利用 SQL 语言对数据进行查询、汇总、分析。同时，这个语言也    允许熟悉 MapReduce 开发者们开发自定义的mappers和reducers来处理内建的    mappers和reducers无法完成的复杂的分析工作。Hive还允许用户编写自己定义的    函数UDF，用来在查询中使用。Hive中有3种UDF：User Defined Functions（UDF）、    User Defined Aggregation Functions（UDAF）、User Defined Table Generating     Functions（UDTF）。也就是说对存储在HDFS中的数据进行分析和管理，我们不想    使用手工，我们建立一个工具吧，那么这个工具就可以是Hive。<br>2.2 原理<br>Hive构建在Hadoop之上<br>（1）HQL中对查询语句的解释、优化、生成查询计划是由Hive完成的<br>（2）所有的数据都是存储在Hadoop中的HDFS上<br>（3）查询计划被转化为MapReduce任务，在Hadoop中执行（有些查询没有MR        任务，如：select * from table）<br>（4）Hadoop和Hive都是用UTF-8编码的</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2.2-4.jpg" alt="alt"></p>
<p>流程大致步骤为：</p>
<ol>
<li>用户提交查询等任务给Driver。</li>
<li>编译器获得该用户的任务Plan。</li>
<li>编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息。</li>
<li>编译器Compiler得到元数据信息，对任务进行编译，先将HiveQL转换为抽象语法    树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询    计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。</li>
<li>将最终的计划提交给Driver。</li>
<li>Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker    或者SourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作。</li>
<li>获取执行的结果。</li>
<li>取得并返回执行结果。</li>
</ol>
<p>创建表时：<br>解析用户提交的Hive语句–&gt;对其进行解析–&gt;分解为表、字段、分区等Hive对象。根    据解析到的信息构建对应的表、字段、分区等对象，从SEQUENCE_TABLE中获取构    建    对象的最新的ID，与构建对象信息（名称、类型等等）一同通过DAO方法写入元数    据库的表中，成功后将SEQUENCE_TABLE中对应的最新ID+5.实际上常见的RDBMS    都    是通过这种方法进行组织的，其系统表中和Hive元数据一样显示了这些ID信息。通    过这些元数据可以很容易的读取到数据。</p>
<p>Hive编译过程为：<br>基本流程为：将HiveQL转化为抽象语法树再转为查询块然后转为逻辑查询计划再转为    物理查询计划最终选择最佳决策的过程。</p>
<p>优化器的主要功能：</p>
<ol>
<li>将多Multiple join 合并为一个Muti-way join</li>
<li>对join、group-by和自定义的MapReduce操作重新进行划分。</li>
<li>消减不必要的列。</li>
<li>在表的扫描操作中推行使用断言。</li>
<li>对于已分区的表，消减不必要的分区。</li>
<li>在抽样查询中，消减不必要的桶。</li>
<li>优化器还增加了局部聚合操作用于处理大分组聚合和增加再分区操作用于处理不对    称的分组聚合。</li>
</ol>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2.2-7.jpg" alt="alt"></p>
<p>2.3 Hive的数据类型<br>Hive支持原子和复杂数据类型，原子数据类型包括：数据值、布尔类型、字符串类型等，复杂的类型包括：Array、Map和Struct。其中Array、Map和java中的Array和Map是相似的，Struct和C语言中的Struct相似。<br>例如：</p>
<p>create table test(<br>col1 Array<int>,<br>col2 Map&lt;String,int&gt;,<br>col3 Struct&lt;a:String,b:int,c:Double&gt;<br>);</int></p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2.3.jpg" alt="alt"></p>
<p>注意：</p>
<ol>
<li>原子数据类型是可以进行隐式的转换的，例如tinyInt类型会自动转为Int类型但是不能由int自动转为tinyInt类型。</li>
<li>所有的整数类型、Float和String类型都可以转换为Double类型。</li>
<li>TinyInt、SmallInt、Int都可以转为Float类型。</li>
<li>Boolean 类型不可以转换为其他的任何类型。</li>
<li>可以通过使用Cast操作显示的进行数据转换，例如Cast(‘1’ as int);将字符串转为整型，如果强制转换失败如：Cast(‘X’ as int);表达式返回的是NULL;</li>
</ol>
<p>第3节 Hive的特点</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2.3-3.jpg" alt="alt"><br>Hive中有两种表：外部表和内部表（managed and external）。可以通过desc formatted table_name 命令来查看表的信息，来辨别表是外部表还是内部表。 在Hive默认创建到表是内部表，外部表创建需要加EXTERNAL 命令，如：CREATE EXTERNAL table_name。<br>   <br>内部表的文件，元数据和统计信息等由Hive进行管理，一般被存储在hive.metastore.warehouse.dir 目录下，当表被删除或者分区被删除，相对应的数据和元数据就会被删除。一般用来当做临时表。外部表与内部表相反，可以指定location，可以不基于Hive来操作外部表文件。当表被删除或者分区被删除时对应的数据还会存在。只是Hive删除了其元信息,表的数据文件依然存在于文件系统中。若是表被删除，可以重新建这个表，指定location到数据文件处，然后通过msck repair table table_name命令刷新数据的元信息到Hive中，也就是恢复了数据。</p>
<p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<p>第4节 操作<br>4.1 进入hive<br>hive</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.1.jpg" alt="alt"></p>
<p>4.2 创建数据库<br>create database test_database;</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.2.jpg" alt="alt"><br>4.3 显示数据库<br>show databases；</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.3.jpg" alt="alt"><br>4.4 使用数据库<br>use test_test_database;</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.4.jpg" alt="alt"></p>
<p>4.5 显示表<br>show tables;</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.5.jpg" alt="alt"></p>
<p>4.6 创建表</p>
<p>create table tables;</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.6.jpg" alt="alt"><br>4.7 加载数据（注意加载的数据一定要在文件夹里）</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.7.jpg" alt="alt"><br>4.8 查询数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4.8.jpg" alt="alt"></p>
<p>第5节 练习</p>
<p>1.1    习题</p>
<ol>
<li>创建一张内部表t_order，原始文件分隔符为’\t’，并加载数据。原始数据为：</li>
</ol>
<p>1    Zhangsan<br>2    Lisi<br>3    Wangwu</p>
<p>展示数据库，并使用创建的数据库</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/1-1.jpg" alt="alt"><br>创建数据表t_order，并查询数据表的内容为空</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/1-2.jpg" alt="alt"><br>查看将要传入的文件</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/1-3.jpg" alt="alt"><br>导入文件并查看数据表内容</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/1-4.jpg" alt="alt"><br>查询ID看一下结果</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/1-5.jpg" alt="alt"></p>
<p>查看hadoop平台是否有这个文件</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/1-6.jpg" alt="alt"></p>
<ol start="2">
<li>创建一张外部表order_log，原始文件分隔符为’\t’，并加载数据。原始数据为：<br>1    90ab176    26    2    3    1    1    28.20    2    2016-11-24 08:36:01    2016-11-24 08:36:01<br>2    73ac321    18    3    2    2    2    33.10    2    2016-11-24 08:37:10    2016-11-24 08:37:10<br>3    70dc236    16    3    2    2    1    39.10    3    2016-11-24 08:39:03    2016-11-24 08:39:03<br>查看文件内容</li>
</ol>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-1.jpg" alt="alt"><br>创建一个地址并给满权限</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2.jpg" alt="alt"><br>创建一个外部表order_log并查看表信息</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-3.jpg" alt="alt"><br>传入数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-4.jpg" alt="alt"><br>查看表数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-5.jpg" alt="alt"><br>测试一下查看a属性</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-6.jpg" alt="alt"><br>Hadoop平台上文件</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-7.png" alt="alt"></p>
<ol start="3">
<li>创建一张内部分区表member_address，原始文件分隔符为’\t’，并加载数据。原始数据为：<br>1    102    heilongjiang    daqing    a    b    18068430051    c    2016-11-24 08:23:01    2016-11-24 08:23:01<br>2    103    heilongjiang    daqing    a    b    18068430051    c    2016-11-24 08:23:01    2016-11-24 08:23:01<br>查看文件内容</li>
</ol>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/3-1.png" alt="alt"><br>创建内部分区表member_address</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/3-2.png" alt="alt"><br>查看内部分区表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/3-3.png" alt="alt"><br>传入数据文件</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/3-4.png" alt="alt"><br>查询内部分区表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/3-5.png" alt="alt"><br>Hadoop平台数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/3-6.png" alt="alt"></p>
<ol start="4">
<li>创建一张外部分区表ext_member_address，原始文件分隔符为’\t’，并加载数据。原始数据为：<br>1    102    heilongjiang    daqing    a    b    18068430051    c    2016-11-24 08:23:01    2016-11-24 08:23:01<br>2    103    heilongjiang    daqing    a    b    18068430051    c    2016-11-24 08:23:01    2016-11-24 08:23:01</li>
</ol>
<p>创建外部分区表ext_member_address</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4-1.png" alt="alt"><br>查看数据表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4-2.png" alt="alt"><br>创建分区</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4-3.png" alt="alt"><br>传入数据并查看数据表内容</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4-4.png" alt="alt"><br>Hadoop平台</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/4-5.png" alt="alt"></p>
<ol start="5">
<li>创建一张桶表bk_order_log，通过查询表order_log加载数据;<br>创建一个桶表</li>
</ol>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/5-1.png" alt="alt"><br>传入数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/5-2.png" alt="alt"><br>查看表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/5-3.png" alt="alt"><br>Hadoop平台</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/5-4.png" alt="alt"></p>
<ol start="6">
<li>使用Hive进行数据分析例子，WordCount，原始文件分隔符为’\t’。原始数据为：<br>wc1.txt<br>username    zhaoming<br>username    zhaopeng<br>username    lisi<br>username    zhangsan<br>wc2.txt<br>username    sunang<br>username    caizhili</li>
</ol>
<p>创建一个目录并上传文件</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/6-1.png" alt="alt"><br>创建数据表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/6-2.png" alt="alt"><br>上传数据并查看</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/6-3.png" alt="alt"><br>处理数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/6-4.png" alt="alt"><br>查看数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/6-5.png" alt="alt"><br>Hadoop平台</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/6-6.png" alt="alt"></p>
<p>2.1    习题<br>student表结构<br>id         int<br>name    string<br>sex        string<br>age        int</p>
<p>stu_class表结构<br>name      string<br>cource      string</p>
<p>student数据</p>
<p>1,李勇,男,20<br>2,刘晨,女,19<br>3,王敏,女,22<br>4,张立,男,19<br>5,刘刚,男,18<br>6,孙庆,男,23<br>7,易思玲,女,19<br>8,李娜,女,18<br>9,梦圆圆,女,18<br>10,孔小涛,男,19<br>11,包小柏,男,18<br>12,孙花,女,20<br>13,冯伟,男,21<br>14,王小丽,女,19<br>15,王君,男,18<br>16,钱国,男,21<br>17,王风娟,女,18<br>18,王一,女,19<br>19,邢小丽,女,19<br>20,赵钱,男,21<br>21,周二,男,17<br>22,郑明,男,20</p>
<p>stu_class数据</p>
<p>李勇,CS<br>刘晨,IS<br>王敏,MA<br>张立,IS<br>刘刚,MA<br>孙庆,CS<br>易思玲,MA<br>李娜,CS<br>梦圆圆,MA<br>孔小涛,CS<br>包小柏,MA<br>孙花,CS<br>冯伟,CS<br>王小丽,CS<br>王君,MA<br>钱国,MA<br>王风娟,IS<br>王一,IS<br>邢小丽,IS<br>赵钱,IS<br>周二,MA<br>郑明,MA</p>
<ol>
<li>将学生表和学生课程表在hadoop平台中创建（使用Hive）；<br>Student表</li>
</ol>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-1-1.png" alt="alt"><br>上传数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-1-2.png" alt="alt"><br>Stu_class表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-1-3.png" alt="alt"><br>上传数据</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-1-4.png" alt="alt"><br>2. 显示学生信息（student）和学生课程信息（stu_class）；<br>Student表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-1.png" alt="alt"><br>Stu_class表</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-2.png" alt="alt"><br>3. 找到学生表（student）中梦圆圆的信息；</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-3.png" alt="alt"><br>4. 找到学生表中年级在18岁和19岁的学生；</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-4.png" alt="alt"><br>5. 找出所有男生的信息；</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-5.png" alt="alt"><br>6. 找出所有女生的信息；</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-6.png" alt="alt"><br>7. 显示两张表中所有同学的信息（包括id,name,sex,age,cource）。</p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-7.png" alt="alt"></p>
<p><img src="/LY4E999.github.io/2020/06/20/Hive/2-2-8.png" alt="alt"></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
</search>
